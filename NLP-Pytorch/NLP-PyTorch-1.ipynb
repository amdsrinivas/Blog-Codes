{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Exploring the [data](https://nlp.stanford.edu/projects/snli/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = 'snli_1.0_train.jsonl'\n",
    "COLUMNS = ['gold_label', 'sentence1', 'sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(INPUT_FILE, lines = True)\n",
    "df = df[COLUMNS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('gold_label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[ df['gold_label'] != '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('gold_label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing pre-trained embeddings : [GLOve](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_embeddings(path):\n",
    "    vocab = []\n",
    "    idx = 0\n",
    "    lookup = {}\n",
    "    vectors = []\n",
    "    with open(path, 'rb') as f:\n",
    "        for l in f:\n",
    "            try:\n",
    "                line = l.decode().split()\n",
    "                word = line[0]\n",
    "                vect = np.array(line[1:]).astype(np.float32) # import numpy as np\n",
    "                vocab.append(word)\n",
    "                vectors.append(vect)\n",
    "                lookup[word] = idx\n",
    "                idx += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)    \n",
    "    return vocab, lookup, vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(vocab, lookup, vectors):\n",
    "    num_embeddings = len(vocab) + 2\n",
    "    embedding_dim = len(vectors[0])\n",
    "    weights_matrix = np.zeros( (num_embeddings, embedding_dim) )\n",
    "    unknown_index = len(vocab)\n",
    "    padding_index = unknown_index + 1\n",
    "    for word in vocab:\n",
    "        index = lookup[word]\n",
    "        weights_matrix[index] = vectors[index]\n",
    "    weights_matrix[unknown_index] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n",
    "    weights_matrix[padding_index] = np.zeros( (embedding_dim,))\n",
    "    print(weights_matrix.shape)\n",
    "    return weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_PATH = 'glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, lookup, vectors = process_embeddings(GLOVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm = build_embedding_matrix(vocab, lookup, vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a model with pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_weigths_matrix):\n",
    "        super(model, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) # ( vocab_size, embedding_dimension )\n",
    "        self.embedding.load_state_dict({'weight': torch.tensor(embedding_weigths_matrix, dtype=torch.float64)})\n",
    "        self.embedding.requires_grad = False\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return self.embedding(inputs)\n",
    "\n",
    "_in = torch.tensor([[1,2,3], [0,4,3]])\n",
    "print('Inputs shape : ', _in.shape)\n",
    "_out = model(wm.shape[0], wm.shape[1], wm)(_in)\n",
    "print('Outputs shape : ', _out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stitching it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = set([_t for _t in string.punctuation]) # import string\n",
    "stop_words = set(stopwords.words('english')) # from nltk.corpus import stopwords\n",
    "stop_words = stop_words.union(puncts)\n",
    "\n",
    "def tokenize(sentence, sequence_length):\n",
    "    tokens = []\n",
    "    sentence = sentence.lower()\n",
    "    pad_token = len(vocab) + 1\n",
    "    for _tok in word_tokenize(sentence):\n",
    "        if _tok not in stop_words:\n",
    "            if _tok in vocab:\n",
    "                tokens.append(lookup[_tok])\n",
    "            else:\n",
    "                tokens.append(len(vocab))\n",
    "    tokens = tokens + [pad_token for i in range(sequence_length-len(tokens))]\n",
    "    return tokens[:sequence_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(df['sentence1'][0], sequence_length=15)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_in = torch.tensor([tokens])\n",
    "print('Inputs shape : ', _in.shape)\n",
    "_out = model(wm.shape[0], wm.shape[1], wm)(_in)\n",
    "print('Outputs shape : ', _out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('venv': virtualenv)",
   "language": "python",
   "name": "python37464bitvenvvirtualenv3a155fc772ab420983d82227381890d1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
